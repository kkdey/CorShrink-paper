
\section{Introduction}
Estimating the covariance or correlation matrix of variables is a common practice among researchers interested in a broad spectrum of statistical applications, ranging from understanding the relationship among variables, performing classification or regression and even form groups or clusters or features. The most common choice for an estimator is the sample covariance or correlation matrix which is also the Maximum Likelihood estimate. While this estimator works well for $ n > p$ scenarios, it has extremely high approximation error due to its low rank structure when $ n << p$. 

In 2003, Ledoit and Wolf proposed an estimator that is well conditioned and has much lesser approximation error than the sample correlation matrix, especially under $n <<p$ scenarios \cite{Ledoit2003} \cite{Ledoit2004}. This approach was further generalized by Sch\"{a}fer and Strimmer (2005) \cite{Schafer2005} \cite{Schafer2005b}, who besides proposing new shrinkage estimators, also provided  analytic calculation of the optimal shrinkage intensity. The idea was to fit a convex combination of the empirical sample covariance matrix (S) along with a chosen target matrix T, which can be chosen to be an  identity matrix or constant correlation matrix. The mixing proportion $\delta$ in the convex combination $\delta T + (1- \delta) S$ is usually selected to minimize the expected error of approximation of the shrunken estimate. The above methods used a single target for shrinking, but a multi-target covariance shrinkage approach was recently proposed - see Lancewicki and Aladjem  (2014) \cite{Lancewiki2014}. 

In this paper, we propose three versions of an alternative method called \textit{CorShrink} which assumes multiple targets 
$T_1$, $T_2$, $\cdots$, $T_K$, all of which are noisy versions of the identity matrix and the noise variation increases with each $k=1,2,\cdots, K$. We adaptively determine the amount of shrinkage by optimally determining the shrinkage weights for each target and selecting the set of targets to cover the range of variation of the data well. We explain the noise structure and the model fit in greater details in the next section.  We also perform comparisons of our model performance with respect to the Sch\"{a}fer and Strimmer approach (Sch\"{a}fer and Strimmer (2005) \cite{Schafer2005}) and the Graphical LASSO algorithm developed by Friedman et al (2008) \cite{Friedman2008} for sparse representation of the correlation and primarily inverse correlation matrices used in building causal networks. We show that  \textit{CorShrink} performs better than the Sch\"{a}fer and Strimmer shrinkage in terms of flexibility in choosing the amount of shrinkage when $ n << p$, and both \textit{CorShrink} and Sch\"{a}fer-Strimmer method perform much better as correlation shrinkage methods compared to GLASSO. We also show an application our method on a single cell RNA-seq data from mouse pre-implantation embryos due to Deng et al (2014) \cite{Deng2014}.


